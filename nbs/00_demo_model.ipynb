{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp demo.version_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "> API details.\n",
    "\n",
    "This is just a dummy set of models, so I can check dependency tracking. It's from the [sklearn documentation](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html?highlight=predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from stack.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def load_if_present(path, mode='rb'):\n",
    "    if path is None: return None\n",
    "    if not Path.exists(path): return None\n",
    "    return load(open(path, mode))\n",
    "\n",
    "def dump_if_path(o, path=None, mode='wb'):\n",
    "    if path is not None: dump(o, open(path, mode))\n",
    "    return o\n",
    "\n",
    "def get_scaler(X=None, fn=StandardScaler, path=None):\n",
    "    scaler = load_if_present(path)\n",
    "    if not X is None:\n",
    "        scaler = fn()\n",
    "        scaler.fit(X)\n",
    "    if not path is None: dump(scaler, open(path, 'wb'))\n",
    "    return scaler\n",
    "        \n",
    "def generate_data(path=None):\n",
    "    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=1, n_clusters_per_class=1)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    ds = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "    X, y = ds\n",
    "\n",
    "    scaler = get_scaler(X=X, path=path)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    return X, y, scaler\n",
    "\n",
    "def split_data(X, y, test_size=.4, **kwargs):\n",
    "    return train_test_split(X, y, test_size=test_size, **kwargs)\n",
    "\n",
    "def train_model(X_train, y_train, path=None):\n",
    "    clf = load_if_present(path)\n",
    "    if clf is None:\n",
    "        clf = SVC(gamma=2, C=1, probability=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "        dump_if_path(clf, path=path)\n",
    "    return clf\n",
    "\n",
    "def predict(params):\n",
    "    root = Path('../tmp')\n",
    "    scaler_path = root/'scaler.pkl'\n",
    "    clf_path = root/'clf.pkl'\n",
    "    clf = load_if_present(clf_path)\n",
    "    scaler = load_if_present(scaler_path)\n",
    "    params = np.asarray(params).reshape(1, -1)\n",
    "    X = scaler.transform(params)\n",
    "    choice = clf.predict(X)[0]\n",
    "    probabilities = clf.predict_proba(X)[0]\n",
    "    return choice, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_if_exists(path):\n",
    "    if not Path.exists(path): return None\n",
    "    Path.unlink(path)\n",
    "\n",
    "root = Path('../tmp')\n",
    "test_file = root/'test_file'\n",
    "rm_if_exists(test_file)\n",
    "\n",
    "# There is no test file currently\n",
    "o = dict(a=1)\n",
    "dump_if_path(o, path=test_file)\n",
    "assert Path.exists(test_file)\n",
    "\n",
    "# Just wrote the test file\n",
    "o1 = load_if_present(test_file)\n",
    "assert o1['a'] == 1\n",
    "rm_if_exists(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "root = Path('../tmp')\n",
    "scaler_path = root/'scaler.pkl'\n",
    "model_path = root/'clf.pkl'\n",
    "\n",
    "X, y, scaler = generate_data(path=scaler_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, random_state=42)\n",
    "clf = train_model(X_train, y_train, path=model_path)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)\n",
    "assert score > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1.2, -1]\n",
    "cls, probabilities = predict(a)\n",
    "i = 1 - cls\n",
    "assert probabilities[cls] > probabilities[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Sense\n",
    "\n",
    "I think I'm OK with this for now:\n",
    "\n",
    "* It has a few functions that will eventually get moved into a utility module.\n",
    "* It can make a prediction from a single set of parameters.\n",
    "* It scales the parameters (using just local files for now)\n",
    "\n",
    "### Now What?\n",
    "\n",
    "Now that I have a trained model and data, but I don't have the Docker setup, MinIO, or DVC, I'm not sure where I want to stick these. I'm going to export the building and predicting functions into a library, and go from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
